{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.pyenv/versions/3.10.15/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>quote_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15225446003</td>\n",
       "      <td>548</td>\n",
       "      <td>l appliqu etre un peu bizarr , pourquoi pas si...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1840193003</td>\n",
       "      <td>548</td>\n",
       "      <td>mais cel l avoir repulp un peu .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4603383003</td>\n",
       "      <td>548</td>\n",
       "      <td>point fort     odeur     facil d utilis     po...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9409317003</td>\n",
       "      <td>548</td>\n",
       "      <td>dior avoir just chang le packaging ce derni an...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9063266003</td>\n",
       "      <td>548</td>\n",
       "      <td>mais cel demand un peu plus de temp que de s a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12584</th>\n",
       "      <td>13640720003</td>\n",
       "      <td>548</td>\n",
       "      <td>a voir pour le autr essenc ... pas d' odeur de...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12585</th>\n",
       "      <td>9803863003</td>\n",
       "      <td>548</td>\n",
       "      <td>le pomp distribu un quantit suffis de produit .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12586</th>\n",
       "      <td>8098980003</td>\n",
       "      <td>548</td>\n",
       "      <td>en un clic on arriv a obten un quantit ampleme...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12587</th>\n",
       "      <td>9180479003</td>\n",
       "      <td>548</td>\n",
       "      <td>le bross etre gener et ne depos que le quantit...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12588</th>\n",
       "      <td>6727790003</td>\n",
       "      <td>548</td>\n",
       "      <td>je me regal     point faibl     ne pas mettr t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12589 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          quote_id  topic_id  \\\n",
       "0      15225446003       548   \n",
       "1       1840193003       548   \n",
       "2       4603383003       548   \n",
       "3       9409317003       548   \n",
       "4       9063266003       548   \n",
       "...            ...       ...   \n",
       "12584  13640720003       548   \n",
       "12585   9803863003       548   \n",
       "12586   8098980003       548   \n",
       "12587   9180479003       548   \n",
       "12588   6727790003       548   \n",
       "\n",
       "                                              quote_text  label  \n",
       "0      l appliqu etre un peu bizarr , pourquoi pas si...   True  \n",
       "1                       mais cel l avoir repulp un peu .  False  \n",
       "2      point fort     odeur     facil d utilis     po...  False  \n",
       "3      dior avoir just chang le packaging ce derni an...  False  \n",
       "4      mais cel demand un peu plus de temp que de s a...  False  \n",
       "...                                                  ...    ...  \n",
       "12584  a voir pour le autr essenc ... pas d' odeur de...  False  \n",
       "12585    le pomp distribu un quantit suffis de produit .   True  \n",
       "12586  en un clic on arriv a obten un quantit ampleme...   True  \n",
       "12587  le bross etre gener et ne depos que le quantit...   True  \n",
       "12588  je me regal     point faibl     ne pas mettr t...   True  \n",
       "\n",
       "[12589 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=\"itg-cldataops-gbl-ww-pd\")\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT quote_id, topic_id, quote_text_normalized as quote_text, label\n",
    "    FROM `itg-cldataops-gbl-ww-pd.bta_consumerloop_bqdset_labelled_data_eu_pd.tbl_topics_dataset_2024_08_30`\n",
    "    WHERE country_id=3 and split=\"train\" and topic_id in (548, 558)\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=df[\"topic_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_id</th>\n",
       "      <th>quote_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>quote_associated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>mais c etre un pet quantit qui couvr beaucoup ! !</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>206</td>\n",
       "      <td>le instruct disent d utilis un pet quantit , c...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>en le voir deven bleu , vous pouvoir voir si l...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208</td>\n",
       "      <td>j util un pet quantit et cel avoir aid a gard ...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209</td>\n",
       "      <td>mais ensuit j avoir achet , et croi moi , vous...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12560</th>\n",
       "      <td>379580006001</td>\n",
       "      <td>quand j avoir ouvr le boit , j etre excit , je...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12561</th>\n",
       "      <td>384072502001</td>\n",
       "      <td>mais j avoir avoir de mal a comprendr comment ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12562</th>\n",
       "      <td>405098231001</td>\n",
       "      <td>mon seul problem etre que je trouv vrai diffic...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12563</th>\n",
       "      <td>414014706001</td>\n",
       "      <td>je devoir prevoir 20 minut supplementair lorsq...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12564</th>\n",
       "      <td>618572126001</td>\n",
       "      <td>le tub de conditionneur dan le deux boit etre ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12565 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           quote_id                                         quote_text  \\\n",
       "0               200  mais c etre un pet quantit qui couvr beaucoup ! !   \n",
       "1               206  le instruct disent d utilis un pet quantit , c...   \n",
       "2               207  en le voir deven bleu , vous pouvoir voir si l...   \n",
       "3               208  j util un pet quantit et cel avoir aid a gard ...   \n",
       "4               209  mais ensuit j avoir achet , et croi moi , vous...   \n",
       "...             ...                                                ...   \n",
       "12560  379580006001  quand j avoir ouvr le boit , j etre excit , je...   \n",
       "12561  384072502001  mais j avoir avoir de mal a comprendr comment ...   \n",
       "12562  405098231001  mon seul problem etre que je trouv vrai diffic...   \n",
       "12563  414014706001  je devoir prevoir 20 minut supplementair lorsq...   \n",
       "12564  618572126001  le tub de conditionneur dan le deux boit etre ...   \n",
       "\n",
       "       labels quote_associated  \n",
       "0      [0, 0]           [1, 0]  \n",
       "1      [0, 0]           [1, 0]  \n",
       "2      [0, 0]           [1, 0]  \n",
       "3      [0, 0]           [1, 0]  \n",
       "4      [0, 0]           [1, 0]  \n",
       "...       ...              ...  \n",
       "12560  [0, 1]           [0, 1]  \n",
       "12561  [0, 1]           [0, 1]  \n",
       "12562  [0, 0]           [0, 1]  \n",
       "12563  [0, 1]           [0, 1]  \n",
       "12564  [0, 1]           [0, 1]  \n",
       "\n",
       "[12565 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['label'] == True]\n",
    "\n",
    "# Grouper le DataFrame\n",
    "grouped_df = df.groupby('quote_id').agg({\n",
    "    'topic_id': lambda x: set(x),\n",
    "    'quote_text': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Cr√©er le DataFrame avec les vecteurs binaire\n",
    "mlb = MultiLabelBinarizer()\n",
    "topic_matrix = mlb.fit_transform(grouped_df['topic_id'])\n",
    "\n",
    "topic_df = pd.DataFrame(topic_matrix, columns=mlb.classes_)\n",
    "\n",
    "# Cr√©er la colonne 'labels'\n",
    "true_labels = set(zip(filtered_df['quote_id'], filtered_df['topic_id']))\n",
    "\n",
    "labels_matrix = [\n",
    "    [1 if (quote_id, topic) in true_labels else 0 for topic in mlb.classes_]\n",
    "    for quote_id in grouped_df['quote_id']\n",
    "]\n",
    "\n",
    "grouped_df['labels'] = labels_matrix\n",
    "\n",
    "\n",
    "# Cr√©er la colonne 'quote_associated'\n",
    "all_combinations = set(zip(df['quote_id'], df['topic_id']))\n",
    "\n",
    "associated_matrix = [\n",
    "    [1 if (quote_id, topic) in all_combinations else 0 for topic in mlb.classes_]\n",
    "    for quote_id in grouped_df['quote_id']\n",
    "]\n",
    "\n",
    "grouped_df['quote_associated'] = associated_matrix\n",
    "\n",
    "\n",
    "result_df = grouped_df[['quote_id', 'quote_text', 'labels', 'quote_associated']]\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_961/127724474.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['labels'] = result_df['labels'].apply(lambda x: [float(i) for i in x])\n",
      "/tmp/ipykernel_961/127724474.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['quote_associated'] = result_df['quote_associated'].apply(lambda x: [float(i) for i in x])\n"
     ]
    }
   ],
   "source": [
    "result_df['labels'] = result_df['labels'].apply(lambda x: [float(i) for i in x])\n",
    "result_df['quote_associated'] = result_df['quote_associated'].apply(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Val Split\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    result_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=result_df['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/user/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = 'microsoft/deberta-v3-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7defea06939949588178756fce00c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee653ef64f44b14ba77d07dd29c42fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(row):\n",
    "    tokenized_example = tokenizer(row[\"quote_text\"], truncation=True, max_length=512)\n",
    "    tokenized_example[\"labels\"] = row[\"labels\"]\n",
    "    tokenized_example[\"quote_associated\"] = row[\"quote_associated\"]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dataset_train = train_dataset.map(preprocess_function)\n",
    "tokenized_dataset_val = val_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    scores_dict = {}\n",
    "\n",
    "    for index, topic in enumerate(topics):\n",
    "        topic_predictions = predictions[:, index].reshape(-1)\n",
    "        topic_labels = labels[:, index].reshape(-1)\n",
    "        \n",
    "        precision = precision_score(topic_labels, topic_predictions, zero_division=0)\n",
    "        recall = recall_score(topic_labels, topic_predictions, zero_division=0)\n",
    "        fbeta = fbeta_score(topic_labels, topic_predictions, beta=0.33, zero_division=0)\n",
    "        f1 = f1_score(topic_labels, topic_predictions, zero_division=0)\n",
    "\n",
    "        topic_str = str(topic)\n",
    "\n",
    "        scores_dict[topic_str] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'fbeta': fbeta,\n",
    "        'f1' : f1\n",
    "    }\n",
    "    \n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        print(inputs.keys())\n",
    "        \n",
    "        labels = inputs.get(\"labels\")\n",
    "        quote_associated = inputs.get(\"quote_associated\")\n",
    "                \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels.float(), reduction='none')\n",
    "        \n",
    "        weighted_loss = bce_loss * quote_associated\n",
    "\n",
    "        loss = weighted_loss.mean()\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/user/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 48\u001b[0m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomLossTrainer(\n\u001b[1;32m     36\u001b[0m        model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     37\u001b[0m        args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m        optimizers\u001b[38;5;241m=\u001b[39m(optimizer, scheduler)\n\u001b[1;32m     44\u001b[0m    )\n\u001b[1;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/lib/python3.10/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mCustomLossTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     12\u001b[0m bce_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, labels\u001b[38;5;241m.\u001b[39mfloat(), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m \u001b[43mbce_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquote_associated\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m weighted_loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path, \n",
    "    num_labels=len(topics),\n",
    "    problem_type = \"multi_label_classification\",\n",
    "    hidden_dropout_prob=0.2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"weighted_loss\",\n",
    "   learning_rate=1e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=35,\n",
    "   weight_decay=0.1,\n",
    "   evaluation_strategy=\"epoch\", #essayer avec step\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    "   logging_dir='weighted_loss/logs',  # R√©pertoire de logs pour TensorBoard\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,  # Enregistrer les logs d√®s la premi√®re √©tape\n",
    ")\n",
    "\n",
    "num_train_epochs = 35\n",
    "total_steps = len(train_dataset) // training_args.per_device_train_batch_size * num_train_epochs\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.1)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=2e-6)\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps= total_steps * 0.1,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "trainer = CustomLossTrainer(\n",
    "       model=model,\n",
    "       args=training_args,\n",
    "       train_dataset=tokenized_dataset_train,\n",
    "       eval_dataset=tokenized_dataset_val,\n",
    "       tokenizer=tokenizer,\n",
    "       #data_collator=data_collator,\n",
    "       compute_metrics=compute_metrics,\n",
    "       optimizers=(optimizer, scheduler)\n",
    "   )\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
